{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f31ca2",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python\n",
    "\n",
    "Natural Language Processing (NLP) is a field of study that focuses on a computer’s ability to interpret human language in order to process, analyze, and extract meaning from large volumes of natural language text data. \n",
    "\n",
    "Previously, you learned how to scrape text from web pages, how to write text into a file, how to read text from a file, and how to modify text values. Now, you will learn how to analyze text using NLP algorithms.\n",
    "\n",
    "In this session, you will learn about natural language processing with Python using the leading NLP library `spaCy`. This library can be used to take on some of the most important tasks in working with text. `spaCy` has quickly become one of the most popular Python frameworks; it is intuitive and it has excellent [documentation](https://spacy.io/usage).\n",
    "\n",
    "We will first look into how we can use `spaCy` for simple text mining processes such as analysis of the grammatical structure of the text, extracting components of the text, lemmatization, etc. Next, we will look into how we can use `spaCy` for more advanced text mining processes such as named entity recognition and sentiment analysis. \n",
    "\n",
    "To use `spaCy`, first import it to your codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d589b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb5c21",
   "metadata": {},
   "source": [
    "For text analysis, `spaCy` relies on statistical models and trained pipelines for various languages. It currently provides support for more than fifty [languages](https://spacy.io/usage/models). A huge community of developers continuously helps with extending the language data of `spaCy` and improving its performance. `spaCy` also has models trained for mixed-language texts. \n",
    "\n",
    "Depending on the language of your text data, its length, and complexity, you can choose and download a pre-trained language model to be used. To download a model you can either use [pip](https://spacy.io/usage/models#download-pip) or [spaCy's download command](https://spacy.io/usage/models#download). \n",
    "\n",
    "Suppose we want to use the `en_core_web_sm` model which is a relatively small model for the English language with the highest efficiency rate. There are other models with higher accuracy but lower efficiency, see [here](https://spacy.io/models/en). Use the following command in your command line to download this language model.\n",
    "\n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "```\n",
    "Once downloaded, you can use the model in your Python script. There is no need to download it again and again.\n",
    "\n",
    "To use a language model in Jupyter Notebook, it must be downloaded every time the kernel is restarted. You can download a language model in the Jupyter Notebook as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0360ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9955c",
   "metadata": {},
   "source": [
    "After the model is downloaded, it can be loaded and used for text analysis. To load a model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_en # loaded model for English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c8304",
   "metadata": {},
   "source": [
    "Once a language model is loaded and is ready to be used, you can feed a text into spaCy's pipeline which, using its language model, first breaks the text into pieces. This process is known as **tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd9e1d",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "\n",
    "Tokenization is breaking the raw text into small chunks called **tokens**. A token may be a word, part of a word, or just characters like punctuation. Tokenization is the first step in the text analysis process. \n",
    "\n",
    "In the example below, we feed a text to spaCy's text analysis pipeline specifying that the loaded English language model should be used to analyze the text. The result of the process is a spaCy document, a `Doc` object, that contains the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d08e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feed the text to spaCy pipeline with the trained English model\n",
    "doc_en = nlp_en(\"In this beautiful sunny day, we learn NLP.\")\n",
    "\n",
    "print(type(doc_en)) # check the returned type\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in doc_en: # print tokens of the text\n",
    "    print(token)\n",
    "    \n",
    "type(token) # check the type of a token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a194f",
   "metadata": {},
   "source": [
    "`spaCy` also returns a list of noun chunks found in the text. You can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”. Let's have a look at the noun chunks `spaCy` detected in our text; you can access them using the `noun_chunks` member of the returned spaCy `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noun chunks:\")\n",
    "for chunk in doc_en.noun_chunks:\n",
    "    print(chunk.text) # get the content of the noun chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011e933",
   "metadata": {},
   "source": [
    "`spaCy` also splits your text into sentences. You can access the recognized sentences using the `sents` member of the spaCy `Doc` object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_en(\"It is Monday and we are at EuroSciPy Conference. Today, I learn about NLP with Spacy. I believe spaCy is super cool\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d85f7d",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "`spaCy` also identifies the role of each token in the sentence. `spaCy` provides a bunch of POS (Part of Speech) tags such as NOUN (noun), PUNCT (punctuation), ADJ (adjective), ADV (adverb), etc. The trained pipeline and statistical models enable `spaCy` to make classification of which tag or label a token belongs to. In simple language, we can say that POS tagging is the process of identifying a word as nouns, pronouns, verbs, adjectives, etc. To learn more about spaCy's part of speech tagging and its list of POS tags, see this [article](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/).\n",
    "\n",
    "As you can see above, `spaCy` has broken our text into tokens. Each token is an instance of class `Token` that has a member `pos_` which contains the POS tag of the token.\n",
    "\n",
    "Let's have a look at POS tags spaCy has given each token of our text\n",
    "> \"In this beautiful sunny day, we learn NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff97db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the tokens and their part of speech tags\n",
    "for token in doc_en:\n",
    "    print(token, \"->\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1269d",
   "metadata": {},
   "source": [
    "You can then filter out any linguistic feature you are interested in, for example all the verbs in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04930bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verbs:\", [token for token in doc_en if token.pos_ == \"VERB\"]) # list of verbs given in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7b5f6",
   "metadata": {},
   "source": [
    "## Dependency Graph\n",
    "\n",
    "`spaCy` also identifies the grammatical dependency between the tokens within the text. For example, which noun an adjective is describing. For that, `spaCy` uses a [dependency parser](https://spacy.io/api/dependencyparser).  \n",
    "\n",
    "### Visualize Dependency Graph\n",
    "\n",
    "To see the dependency between the tokens, we can visualize it using [displacy](https://spacy.io/usage/visualizers) class of `spaCy`.\n",
    "\n",
    "In a Python script, use the `displacy.serve()` function to visualize the dependency graph of the text. This will spin up a simple web server and let you view the result straight from your browser. In your python script, you can visualize the dependency graph using:\n",
    "\n",
    "```\n",
    "displacy.serve(doc_en, style=\"dep\")\n",
    "```\n",
    "\n",
    "Jupyter Notebook is already rendered in a browser, so for displaying the dependency graph within Jupyther notebook, you should use `displacy.render()` function, as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c630d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc_en, style = \"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e213f",
   "metadata": {},
   "source": [
    "The `displacy.render()` function returns the graph in the [SVG](https://en.wikipedia.org/wiki/Scalable_Vector_Graphics), Scalable Vector Graphics, format that can be rendered on web. In order to store the resulting dependency graph, you can store it as a `.svg` file that can be embedded in any webpage and can be viewed using any web browser. \n",
    "\n",
    "In Jupyter Notebook, for SVG to be returned instead of displayed, you should set the parameter `jupyter = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732394d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "svg = spacy.displacy.render(doc_en, style = \"dep\", jupyter = False)\n",
    "with open(\"sentence_dep.svg\", \"w\",  encoding=\"utf-8\") as file:\n",
    "    file.write(svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d793",
   "metadata": {},
   "source": [
    "**Note:** To get the dependency graph returned as SVG in a Python script, there is no need to set `jupyter = False` parameter. You can get the dependency graph as SVG with\n",
    "\n",
    "```\n",
    "svg = spacy.displacy.render(doc_en, style = \"dep\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703dfdd",
   "metadata": {},
   "source": [
    "If you wish to store the dependency graph as an image file, you can easily convert an SVG file to PNG using the `svglib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4851bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this command to install the package in Jupyter Notebook\n",
    "!pip3 install svglib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059580b",
   "metadata": {},
   "source": [
    "The `svg2rlg()` of this package is a python tool to convert SVG files to reportlab graphics. After this conversion, we can save the graphics as an image in file specifying the format like `fmt=PNG`, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPM\n",
    "\n",
    "drawing = svg2rlg(\"sentence_dep.svg\")\n",
    "renderPM.drawToFile(drawing, \"sentence_dep.png\", fmt=\"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32612693",
   "metadata": {},
   "source": [
    "### Example: Analyze a German Text\n",
    "\n",
    "Let's have a look at an example in German. To analyze texts in German, you need a model trained for German language texts. For higher accuracy, I have chosen the `de_core_news_lg`, there are other pre-trained models [available](https://spacy.io/models/de) that you can choose from. Again first, you should download the model and then load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e121774",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec568f",
   "metadata": {},
   "source": [
    "Now we can use this German-language model to analyze a German text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_de = nlp_de(\"Heute ist ein sonniger Tag.\")\n",
    "for token in doc_de:\n",
    "    print(token, token.pos_)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4cb55",
   "metadata": {},
   "source": [
    "Now let's have a look at the dependency graph of this German Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc_de, style = \"dep\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8804d5",
   "metadata": {},
   "source": [
    "**Note:** The German model is not as good as the English one, after all, German has a more complex grammatical structure. The separable verbs in German make things even more complex. The chosen model nonetheless has an accuracy rate of 98% for POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42ae6e",
   "metadata": {},
   "source": [
    "Next analysis task in the default pipeline of spaCy is **lemmatization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7c719",
   "metadata": {},
   "source": [
    "## Lemmatization:\n",
    "\n",
    "Lemmatization is a text normalization technique in the field of Natural Language Processing. It is used to prepare text, words, and documents for further processing. Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. It is one of the most common text pre-processing techniques used in NLP and machine learning in general. Examples of lemmatization:\n",
    "\n",
    "```\n",
    "play, playing, played, plays -> play\n",
    "am, is, are, were, was -> be\n",
    "good, better, best -> good\n",
    "corpora -> corpus\n",
    "```\n",
    "\n",
    "After tokenization and dependency parsing of a text, `spaCy` identifies the lemma of each token (stem of the word). The lemma of a token is added to the `lemma_` member of the `Token` object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865bceb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp_en(\"The striped bats are hanging on their feet for best.\")\n",
    "for token in doc:\n",
    "    print(token, \"-> \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7a561",
   "metadata": {},
   "source": [
    "Let's put the lemmas of tokens together to get the lemmatized form of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0dbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77144b3",
   "metadata": {},
   "source": [
    "Lemmatized form of text is easier to analyze for machines and is widely used in tagging systems and indexing, Web search results, and information retrieval. \n",
    "\n",
    "For some text mining tasks like text classification, it is better to next remove the stop words from the lemmatized text. These are the words that do not add much to the meaning of the document. Generally, the most common stop words used in a text are \"the\", \"is\", \"in\", \"for\", \"where\", \"when\", \"to\", \"at\" etc. If for your text analysis task you need to remove stop words from the text, see this document to learn about [removing stop words using spaCy](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6590b2",
   "metadata": {},
   "source": [
    "In the last step of its default text analysis [pipeline](https://spacy.io/models#design-cnn), spaCy recognizes the named entities given in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453a53e",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "[Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (NER) is probably the first step towards information extraction that seeks to locate named entities given in the text and to classify them into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. \n",
    "\n",
    "It helps with answering real-world questions, such as \n",
    "\n",
    "- How many times a certain organization was mentioned in a text?\n",
    "- Were specific products mentioned in complaints or reviews?\n",
    "- Does the tweet contain the name of a person? does the tweet contain the location of a person?\n",
    "\n",
    "`spaCy` features an extremely fast statistical [entity recognition system](https://spacy.io/usage/linguistic-features#named-entities), that assigns labels to contiguous spans of tokens.  `spaCy` can recognize various types of named entities in a document by asking the model for a prediction. Because models are statistical and strongly dependent on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later depending on your use case.\n",
    "\n",
    "`spaCy` recognizes the named entities in the text and tags them with their types. Using `ents` member of the spaCy `Doc` object, we can access the recognized named entities. For each named entity, its label indicating the type of the entity, as well as the start and end position of that named entity within the text is given: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961b4e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = (\"Over 100 Allied bombers and their crews were interned during the war. \\\n",
    "        Between 1940 and 1945, Switzerland was bombed by the Allies causing \\\n",
    "        fatalities and property damage. \\\n",
    "        Among the cities and towns bombed were Basel, Zurich, Geneva, among others.\")\n",
    "\n",
    "doc_wwII = nlp_en(text)\n",
    "\n",
    "for entity in doc_wwII.ents: # get named entities, their labels, and their positions in the text\n",
    "    print(entity.text, \"->\", entity.label_, \", position:\", \"[\", entity.start_char, \",\", entity.end_char, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f6ed6",
   "metadata": {},
   "source": [
    "The accuracy of the named entity recognition (NER) using the chosen model `en_core_web_sm` is only 85%. For a better accuracy rate, you can use other models such as `en_core_web_trf` which has an accuracy rate of 90% for NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d512d42",
   "metadata": {},
   "source": [
    "### Visualization of Named Entities in Text\n",
    "\n",
    "The entity visualizer method of the `displacy` class, with visualization style parameter `style=\"ent\"`, highlights named entities and their labels in a text. \n",
    "\n",
    "In a Python script, use the` displacy.serve()` function to get the visualization of the text with recognized named entities. It returns the text with tagged named entities as an HTML document. This function will spin up a simple webserver to let you view the result straight from your browser. It can be used as shown below:\n",
    "\n",
    "```\n",
    "displacy.serve(doc_wwII, style=\"ent\")\n",
    "```\n",
    "\n",
    "Jupyter notebook is already running in the browser, and can directly render and display HTML documents. Therefore, to visualize the text with tagged named entities in Jupyter Notebook, you can use the `displacy.render()` function as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fc471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displacy.render(doc_wwII, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9409081",
   "metadata": {},
   "source": [
    "You can store this annotated text with labeled named entities in an HTML file. The labeling of the named entities is defined as CSS (stylesheet) of the HTML document. The resulting HTML document can be opened in any browser and embedded in any website. \n",
    "\n",
    "In Jupyter Notebook, to get the HTML document from the `displacy.render()` function, you should use the `jupyter = False` parameter, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103291ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HTML document with text and tagged named entities\n",
    "html = displacy.render(doc_wwII, style=\"ent\", jupyter = False)\n",
    "\n",
    "# write the HTML document to a new file\n",
    "with open(\"named_entities.html\", \"w\", encoding = \"utf-8\") as output_file:\n",
    "    output_file.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba525945",
   "metadata": {},
   "source": [
    "**Note:** To get the text with labeled named entities returned as HTML in a Python script, you do not need the `jupyter = False` parameter. Use the following command to get the HTML document.\n",
    "\n",
    "```\n",
    "html = spacy.displacy.render(doc_wwII, style = \"ent\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf5897",
   "metadata": {},
   "source": [
    "In the previous session, you learned how to scrape text from web pages using the `requests` and `BeautifulSoup` libraries. You can now use `spaCy` to analyze your scraped text.\n",
    "\n",
    "I have scraped the content of a New York Times [article](https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news) and saved the text in the `nyt_article.txt` file given in the `data` folder.\n",
    "\n",
    "Now let's read the text from that file, and use `spaCy` for the recognition of its named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc017090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read the nyt_article.txt file given in the data folder\n",
    "with open(os.path.join(\"data\", \"nyt_article.txt\"), \"r\") as text_file:\n",
    "    text_en = text_file.read()    \n",
    "text_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc683f7",
   "metadata": {},
   "source": [
    "The text is in English, so we use the English pre-trained model of `spaCy` to detect and visualize the named entities within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_text_doc_en = nlp_en(text_en) \n",
    "\n",
    "spacy.displacy.render(random_text_doc_en, style = \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0acf94",
   "metadata": {},
   "source": [
    "**Note:** As mentioned before, the accuracy of the NER process is dependent on the chosen model and its training samples. Still, the best English pre-trained model of spaCy has an accuracy rate of 90% for NER. The NER algorithms might fail to correctly tag names that can be used for different types of entities; for example Mendoza as a person, and Mendoza which is a city in Argentina. You can however either simply correct the wrong labels by editing the resulting HTML document, or you can train your own model and load it to be used for tokenization and named entity recognition. Guidelines for that can be found [here](https://spacy.io/usage/training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92861a",
   "metadata": {},
   "source": [
    "The text analysis processes mentioned above are all part of the default pipeline of `spaCy`. Once you feed a text into spaCy's algorithm, it performs all these tasks and returns the results. You can furthermore extend spaCy's pipeline to include other natural language processing tasks such as **sentiment analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c61dc",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment analysis (or opinion mining) is a natural language processing (NLP) technique used to determine whether data is positive, negative, or neutral. Sentiment analysis is perhaps one of the most popular applications of NLP, with a vast number of tutorials, courses, and applications that focus on analyzing sentiments of diverse datasets ranging from corporate surveys to movie reviews.\n",
    "\n",
    "[spaCyTextBlob](https://spacy.io/universe/project/spacy-textblob) is a pipeline component that enables sentiment analysis with `spaCy` using the `TextBlob` library in the background. \n",
    "\n",
    "You should first install this pipe from [PyPi](https://pypi.org/project/spacytextblob/):\n",
    "\n",
    "```\n",
    "!pip3 install spacytextblob\n",
    "```\n",
    "\n",
    "\n",
    "It will add the additional extensions `._.polarity`, `._.subjectivity`, and `._.assessments` to a `spaCy` document object.\n",
    "\n",
    "- **Polarity:** The key aspect of sentiment analysis is to analyze a body of text for understanding the opinion expressed by it. Typically, we quantify this sentiment with a positive or negative value, called polarity. The overall sentiment is often inferred as positive, neutral, or negative from the sign of the polarity score. The polarity score is a float within the range [-1.0, 1.0] where -1 is very negative and 1 is very positive.\n",
    "\n",
    "- **Subjectivity:** It is an identifier of how subjective or objective a text is. The subjectivity parameter is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "\n",
    "- **Assessments:** The text components that have affected the sentiment analysis results with their respective polarity and subjectivity scores.\n",
    "\n",
    "To start sentiment analysis using `spaCy` and its additional pipeline component  `spaCyTextBlob`, you should first import `SpacyTextBlob` into your codebase, as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619536c",
   "metadata": {},
   "source": [
    "Then we should add `spacytextblob` pipe to the default pipeline of spaCy using the `add_pip()` method. In this way, we add the functionality for sentiment analysis to our text analysis process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d536d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentiment analysis pipe to the spaCy default pipeline with English language model\n",
    "nlp_en.add_pipe('spacytextblob') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb148a",
   "metadata": {},
   "source": [
    "Now we can perform some simple sentiment analysis on texts. Sentiment analysis can be done at the document level, sentence level, and word level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then, I have a really good day that makes me happy.'\n",
    "doc = nlp_en(text)\n",
    "print('Polarity:', doc._.polarity)  \n",
    "print('Subjectivity:',doc._.subjectivity)\n",
    "print('Assessments:',doc._.assessments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc6bd6",
   "metadata": {},
   "source": [
    "Now let's have a look at the sentiment analysis result of each sentence of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce0e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the sentiment analysis of each sentence:\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    print('Polarity:', sentence._.polarity)  \n",
    "    print('Subjectivity:',sentence._.subjectivity)\n",
    "    print('Assessments:',sentence._.assessments)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3d384",
   "metadata": {},
   "source": [
    "The word-level analysis gives an overview of analysis scores of every single word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    print(word, word._.polarity, word._.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7b526",
   "metadata": {},
   "source": [
    "Text mining is not limited to sentiment analysis and named entity recognition. There is a lot more to say about this topic and there are other Python libraries such as [NLTK](https://www.nltk.org/) that are optimal for advanced text mining processes. An entire course can be dedicated to natural language processing, for now, we suffice to the described processes that are very helpful for basic text analysis with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039de43",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Use `spaCy` with the English model `en_core_web_sm` for the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ecd60",
   "metadata": {},
   "source": [
    "**Exercise 1: (2 Points)**\n",
    "\n",
    "Use `spaCy` to tokenize and parse the following text\n",
    "> When you dive into the sea, you are diving into the origin of us all.\n",
    "\n",
    "Draw the dependency graph of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e468a49",
   "metadata": {},
   "source": [
    "**Exercise 2: (4 Points)** Having the following text:\n",
    "\n",
    "> When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. \"I can tell you many senior CEOs of major American car companies would shake my hand and turn away because I was not worth talking to,\" said Sebastian Thrun, in an interview with Times earlier this week.\n",
    "\n",
    "1. Print the list of noun chunks found in the text\n",
    "2. Print the sentences found in the text.\n",
    "3. Store the lemmatized form of the text in a new file \"thurn_interview_lemma.txt\"\n",
    "4. Get the recognized named entities and store the text with tagged named entities as an HTML file, called \"thurn_interview.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b724e24",
   "metadata": {},
   "source": [
    "**Exercise 3: (4 Points)** Download the CSV file of Trip Advisor hotel reviews from [Kaggle](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews?select=tripadvisor_hotel_reviews.csv), or use the one in the `data` folder. Read the CSV file to a DataFrame that will have two columns `Review` and `Rating`.\n",
    "\n",
    "Taking only the first 1000 rows of this DataFrame,\n",
    "1. Use sentiment analysis to get `Polarity` and `Subjectivity` scores for each review. Create a new DataFrame, `reviews_updated_df` with four columns `Review`, `Rating`, `Polarity`, `Subjectivity`. \n",
    "2. How subjective are the reviews on average? (Hint, print the average of scores in the `Subjectivity` column.)\n",
    "3. Considering the polarity scores of reviews, how many reviews are relatively negative (polarity < -0.3)? Print the number out.\n",
    "4. Write the `reviews_updated_df` DataFrame into an Excel file `hotel_reviews.xlsx`\n",
    "\n",
    "The `Rating` values are between 1 and 5. Comparing the ratings to the `Polarity` scores of reviews, do you think ratings correspond to the actual sentiment expressed in the reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d94777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889b27d",
   "metadata": {},
   "source": [
    "## Literature\n",
    "- [What is text mining?](https://www.linguamatics.com/what-text-mining-text-analytics-and-natural-language-processing)\n",
    "- A simple spaCy tutorial [video](https://www.youtube.com/watch?v=WnGPv6HnBok&t=1653s&ab_channel=Explosion)\n",
    "- More about [sentiment analysis](https://monkeylearn.com/sentiment-analysis/)\n",
    "- More about [sentiment analysis with Python](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
